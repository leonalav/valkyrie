Below Iâ€™ll strip it down to brass tacks and wrap it back up in a neat, telling narrative: what the S5 parallel scan is, why it exists, how itâ€™s formulated, how itâ€™s implemented, its complexity and trade-offs, and all the little practical gotchas the paper documents. Iâ€™ll quote the paperâ€™s appendices and main text where each load-bearing fact comes from so you can follow the source. No fluff â€” poetic but technical.

What the parallel scan in S5 is (short)

S5 uses a parallel scan (all-prefix-sum) to compute the linear recurrence of a diagonalized state-space model:

ğ‘¥
ğ‘˜
=
ğ´
ğ‘¥
ğ‘˜
âˆ’
1
+
ğµ
ğ‘¢
ğ‘˜
x
k
	â€‹

=Ax
kâˆ’1
	â€‹

+Bu
k
	â€‹


after diagonalizing the continuous dynamics 
ğ´
=
ğ‘‰
Î›
ğ‘‰
âˆ’
1
A=VÎ›V
âˆ’1
 and working in the eigenbasis. The scan computes all prefix results in parallel via an associative binary operator, giving the whole sequence 
ğ‘¥
1
:
ğ¿
x
1:L
	â€‹

 with logarithmic parallel depth (in sequence length) and linear total work. 

s5

 

s5

Why S5 uses a parallel scan (motivation)

Efficiency for diagonal dynamics. For a diagonal 
ğ´
A the scan reduces the expensive matrixâ€“matrix work of general parallelization to elementwise (per state) operations. With 
ğ‘ƒ
P latent dims and sequence length 
ğ¿
L, the total work is 
ğ‘‚
(
ğ‘ƒ
ğ¿
)
O(PL) and (with enough processors) parallel time 
ğ‘‚
(
log
â¡
ğ¿
â‹…
ğ‘ƒ
)
O(logLâ‹…P) or more precisely 
ğ‘‚
(
ğ‘ƒ
log
â¡
ğ¿
)
O(PlogL) if one counts per-element costs â€” the paper emphasises the scan becomes practical only after diagonalization. 

s5

Time-domain recurrence (not FFT). S4 used convolution + FFT for offline parallelism. S5 replaces that with a purely recurrent, time-domain parallel scan â€” which also lets S5 handle time-varying SSMs and irregular sampling easily (you can change the discrete step âˆ† per step). 

s5

(Direct citation: Section 2.2 and 3.3 describe exactly this tradeoff.)

The math: how the scan is set up (exactly)

Diagonalize continuous dynamics: 
ğ´
=
ğ‘‰
Î›
ğ‘‰
âˆ’
1
A=VÎ›V
âˆ’1
. Work with

ğ‘¥
~
=
ğ‘‰
âˆ’
1
ğ‘¥
x
~
=V
âˆ’1
x, 
ğµ
~
=
ğ‘‰
âˆ’
1
ğµ
B
~
=V
âˆ’1
B, 
ğ¶
~
=
ğ¶
ğ‘‰
C
~
=CV. Discretize (ZOH) to get 
Î›
Ë‰
Î›
Ë‰
, 
ğµ
Ë‰
B
Ë‰
. 

s5

Construct per-time elements for scan. For each time 
ğ‘˜
k define the element

ğ‘
ğ‘˜
=
(
ğ‘
ğ‘˜
,
ğ‘
,
ğ‘
ğ‘˜
,
ğ‘
)
:
=
(
ğ´
,
â€…â€Š
ğµ
ğ‘¢
ğ‘˜
)
c
k
	â€‹

=(c
k,a
	â€‹

,c
k,b
	â€‹

):=(A,Bu
k
	â€‹

)

(in the diagonalized / discretized domain these are vectors per state). 

s5

Binary associative operator 
âˆ™
âˆ™ used by the scan:

ğ‘
ğ‘–
âˆ™
ğ‘
ğ‘—
:
=
(
ğ‘
ğ‘—
,
ğ‘
â€‰
ğ‘
ğ‘–
,
ğ‘
â€…â€Š
,
â€…â€Š
ğ‘
ğ‘—
,
ğ‘
âŠ—
ğ‘
ğ‘–
,
ğ‘
+
ğ‘
ğ‘—
,
ğ‘
)
q
i
	â€‹

âˆ™q
j
	â€‹

:=(q
j,a
	â€‹

q
i,a
	â€‹

,q
j,a
	â€‹

âŠ—q
i,b
	â€‹

+q
j,b
	â€‹

)

where 
â‹…
â‹… is matrix multiplication, 
âŠ—
âŠ— matrix-vector multiply, and + is vector add. The scan over 
[
ğ‘
1
,
â€¦
,
ğ‘
ğ¿
]
[c
1
	â€‹

,â€¦,c
L
	â€‹

] with this operator returns prefix pairs whose second component is the desired 
ğ‘¥
ğ‘˜
x
k
	â€‹

. The paper proves associativity. 

s5

Intuition / 4-step worked example. For 
ğ¿
=
4
L=4 the paper shows step-by-step how adjacent reductions let you compute in two parallel layers instead of four sequential ones. This is the classic parallel prefix algorithm illustrated concretely in Appendix H. 

s5

Implementation (what the paper ships and why)

The authors include a minimal JAX implementation (Appendix A). Key practical points:

They use jax.lax.associative_scan (JAXâ€™s parallel scan primitive) to run the binary operator over the sequence. The code sets parallel_scan = jax.lax.associative_scan. 

s5

discretize(Lambda, B_tilde, Delta) computes 
Î›
Ë‰
=
exp
â¡
(
Î›
Î”
)
Î›
Ë‰
=exp(Î›Î”) and 
ğµ
Ë‰
=
(
Î›
âˆ’
1
(
Î›
Ë‰
âˆ’
ğ¼
)
)
ğµ
~
B
Ë‰
=(Î›
âˆ’1
(
Î›
Ë‰
âˆ’I))
B
~
 (ZOH discretization expressed elementwise). 

s5

binary_operator in the code implements: return A_j * A_i, A_j * Bu_i + Bu_j (elementwise multiplication across diagonalized state dims). That matches the math: composition of A blocks and accumulation of transformed Bu terms. 

s5

The scheme computes Bu_elements = jax.vmap(lambda u: B_bar @ u)(input_sequence) and then does the associative scan to yield states xs. Finally outputs ys = C_tilde @ xs + D * u then applies nonlinear activation (e.g., GELU) to get layer outputs. 

s5

So if you want to implement S5: diagonalize 
ğ´
A â†’ compute discretized 
Î›
Ë‰
,
ğµ
Ë‰
Î›
Ë‰
,
B
Ë‰
 â†’ compute per-step Bu â†’ associative_scan with the operator â†’ map back with 
ğ¶
~
C
~
. The paperâ€™s code is intentionally compact and directly maps math â†’ code. 

s5

Complexity and resource profile (exact statements)

Total work (offline, parallel): computing the recurrence with a diagonal 
ğ´
A costs 
ğ‘‚
(
ğ‘ƒ
ğ¿
)
O(PL) operations and 
ğ‘‚
(
ğ‘ƒ
ğ¿
)
O(PL) memory (work-efficient scan). The matrix-vector products to form Bu and Cx add 
ğ‘‚
(
ğ‘ƒ
ğ»
ğ¿
)
O(PHL) operations overall. When 
ğ‘ƒ
=
ğ‘‚
(
ğ»
)
P=O(H), S5â€™s total asymptotic cost and memory are the same order as S4â€™s efficient FFT-based convolution (the paper proves Proposition 1).

Parallel time (with many processors): the scan reduces sequential depth to 
ğ‘‚
(
log
â¡
ğ¿
)
O(logL) (with per-state cost factors), so the parallel time is 
ğ‘‚
(
ğ‘ƒ
log
â¡
ğ¿
)
O(PlogL) if you count per-state work, or more compactly 
ğ‘‚
(
log
â¡
ğ‘ƒ
+
log
â¡
ğ¿
)
O(logP+logL) in their ideal processor accounting when enough processors are available. The paper references Blelloch (1990) for the parallel prefix complexity details.

Online per-step cost (recurrent mode): S5 uses 
ğ‘‚
(
ğ‘ƒ
ğ»
+
ğ‘ƒ
)
O(PH+P) operations per step (diagonal multiply + Bu and Cx multiplications). With 
ğ‘ƒ
=
ğ‘‚
(
ğ»
)
P=O(H) this matches S4â€™s per-step complexity of 
ğ‘‚
(
ğ»
2
)
O(H
2
) (S4â€™s DPLR and mixing). 

s5

Practical speedups: their empirical JAX benchmarks show S5 can be faster than S4/S4D for long sequences (Table 4): e.g., on Path-X (length 16,384) S5 shows multi-fold speedups (their best S5 variant was ~4Ã— faster in train step speed relative to S4D baseline in their setup). See Appendix C.2.

Numerical / parameterization details that affect the scan

Diagonalization requirement. Efficient scans require 
ğ´
A diagonal (or block-diagonal with small blocks). S5 parameterizes the model by diagonalizing a chosen initial 
ğ´
A and learns in that basis; discretization uses per-state timescales âˆ†. 

s5

HiPPO / initialization. The HiPPO-LegS matrix (used in S4) is not stably diagonalizable â€” so S5 uses the diagonalization of the HiPPO-N (normal) component as a practical initialization (and can also use block-diagonal HiPPO-N). That preserves the useful long-range dynamics while allowing diagonalization for scanning. This choice is empirically important.

Conjugate symmetry (complex eigenpairs): real matrices have complex conjugate eigenpairs. S5 enforces conjugate symmetry and only stores half the eigenvalues / states to guarantee real outputs and reduce runtime/memory by â‰ˆ2Ã—. This affects how the scan is run (work on complex pairs but store fewer). 

s5

Block-diagonal initialization option: the authors found initializing 
ğ´
A as block-diagonal (several HiPPO-N blocks) sometimes helps and still allows efficient scanning if blocks are small â€” a middle ground between full dense and fully diagonal. 

s5

Proofs, associativity, and correctness (details)

The paper defines the operator and then proves it is associative (Appendix H). Associativity is necessary so associative_scan or any parallel prefix algorithm is correct. The appendices also include the L=4 worked example showing how parallel composition yields prefix states in 
ğ‘‚
(
log
â¡
ğ¿
)
O(logL) rounds instead of 
ğ‘‚
(
ğ¿
)
O(L) rounds.

Practical advantages / special capabilities because of the scan

Time-varying SSMs & irregular samples. Because S5 does time-domain recurrence, you can pass a different discrete step âˆ† (or even different 
ğ´
ğ‘˜
A
k
	â€‹

) at each time step efficiently â€” something the frequency-domain convolution in S4 cannot do without re-computing kernels. S5 uses the same scan machinery but with per-step discretized matrices. This is key for irregularly sampled time series (pendulum experiment in paper).

Simplicity of implementation. the JAX code is compact (Appendix A) and directly uses associative_scan. That lowers engineering friction. 

s5

Limitations, caveats, and gotchas (what the paper warns about)

Diagonalization numerics: you cannot literally use HiPPO-LegSâ€™s full matrix because itâ€™s not stably diagonalizable â€” you must use a normal approximation (HiPPO-N) or a DPLR/block diagonal approximation to be numerically stable. Using the wrong diagonalization will break performance or numeric stability. 

s5

Processor model vs real hardware. The theoretical parallel time assumes many processors (one per stream element). On real GPUs/TPUs there are scheduling and memory tradeoffs; their empirical benchmarks are on JAX with V100 GPUs and show real wins for long sequences, but results depend on hardware, batching, and implementation details. See Appendix C.2 for empirical evidence. 

s5

Complex types & memory layout. Working in the eigenbasis introduces complex values â€” implementation must handle complex dtypes or the conjugate-symmetry trick carefully. 

s5

Everything actionable (if youâ€™re going to implement / experiment)

Diagonalize a stable, normal initialization (HiPPO-N) to get 
Î›
,
ğ‘‰
Î›,V. Initialize 
ğµ
~
=
ğ‘‰
âˆ’
1
ğµ
B
~
=V
âˆ’1
B, 
ğ¶
~
=
ğ¶
ğ‘‰
C
~
=CV.

Use ZOH discretization per state: 
Î›
Ë‰
=
exp
â¡
(
Î›
Î”
)
Î›
Ë‰
=exp(Î›Î”), 
ğµ
Ë‰
=
Î›
âˆ’
1
(
Î›
Ë‰
âˆ’
ğ¼
)
ğµ
~
B
Ë‰
=Î›
âˆ’1
(
Î›
Ë‰
âˆ’I)
B
~
. (See discretize() in Appendix A.) 

s5

Compute per-step Bu_k = B_bar @ u_k for all k (batched vmap), then do associative_scan with binary_operator returning (A_j * A_i, A_j * Bu_i + Bu_j); the second field of scan outputs gives you states x_k. 

s5

Convert back to outputs with 
ğ¶
~
ğ‘¥
ğ‘˜
+
ğ·
ğ‘¢
ğ‘˜
C
~
x
k
	â€‹

+Du
k
	â€‹

, then nonlinearity. 

s5

Enforce conjugate symmetry to halve runtime/memory: parameterize only half the eigenpairs and reflect them. 

s5

Short reading map to the paper (where to look for each major point)

Core idea + diagonalization + why scans: Section 3.2â€“3.3. 

s5

Complexity proof and proposition: Appendix C (C.1 + C.2).

The parallel-scan operator, associativity proof, L=4 example: Appendix H (Background on parallel scans).

JAX code and minimal implementation: Appendix A. 

s5

Initialization (HiPPO-N, conjugate symmetry, timescale sampling): Appendix B (B.1.*).

Final blunt take (no sugar)

The S5 parallel scan is the clean, practical trick that turns an SSM into a time-domain algorithm that is both implementable (short code) and competitive with S4â€™s FFT approach. It hinges on diagonalizing dynamics (or small blocks) and exploiting an associative operator so prefix sums = states. If you diagonalize poorly youâ€™ll wreck stability; if you diagonalize well (HiPPO-N or block HiPPO-N) you get the long-range inductive biases and a fast, flexible runtime that handles variable timesteps. The paper proves the operator, gives worked examples, ships JAX code, and shows empirical speed/accuracy benefits. Read Appendix H + Appendix A + Appendix C in that order if you want to go from concept â†’ code â†’ benchmarks.