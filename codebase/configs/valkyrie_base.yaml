# Valkyrie Base Configuration
# High-performance training for Longformer + S5 on TPU v4-32

# Model Architecture
model:
  # Basic architecture
  vocab_size: 50257
  d_model: 1536
  n_layers: 32
  n_heads: 16
  n_kv_heads: 16  # Set to n_heads for full attention, reduce for GQA
  
  # Position embeddings and RoPE
  original_max_position_embeddings: 4096
  max_position_embeddings: 32768
  rope_theta: 10000.0
  yarn_beta_fast: 32.0
  yarn_beta_slow: 1.0
  
  # Dropout rates
  attn_dropout: 0.0
  resid_dropout: 0.0
  ffn_dropout: 0.1
  
  # Model configuration
  use_bias: false
  layer_norm_eps: 1.0e-5
  initializer_range: 0.02
  
  # S5 State Space Model configuration
  s5_state_dim: 128
  use_s5: true  # Use S5 instead of FFN for long-range dependencies
  
  # Longformer attention configuration
  use_longformer_attention: true
  longformer_window_size: 512
  longformer_global_attention_indices: [0]  # First token is global
  longformer_dilation: null  # Avoid unless custom kernel available
  longformer_chunked: true
  longformer_chunk_size: 512
  longformer_use_full_attention_fallback: true
  longformer_combine_logits: false

# Training Configuration
training:
  # Basic training settings
  total_steps: 100000
  learning_rate: 2.0e-4
  warmup_steps: 5000
  weight_decay: 0.1
  gradient_clipping: 1.0
  gradient_checkpointing: true
  
  # Chunked sequence processing (from output.txt strategy)
  chunk_config:
    chunk_size: 8192
    overlap_size: 512
    min_chunk_size: 1024
    max_chunks_per_doc: 82  # ~657k / 8k
    backprop_chunks: 4
    long_backprop_every: 100
    long_backprop_chunks: 16
  
  # Progressive curriculum (from output.txt)
  curriculum:
    phases:
      - name: "phase0"
        chunk_size: 2048
        backprop_chunks: 2
        max_steps: 5000
        lr: 2.0e-4
      - name: "phase1"
        chunk_size: 8192
        backprop_chunks: 4
        max_steps: 10000
        lr: 1.5e-4
      - name: "phase2"
        chunk_size: 32768
        backprop_chunks: 8
        max_steps: 20000
        lr: 1.0e-4
      - name: "phase3"
        chunk_size: 65536
        backprop_chunks: 16
        max_steps: 50000
        lr: 5.0e-5
  
  # Optimizer configuration
  optimizer:
    type: "adamw"
    b1: 0.9
    b2: 0.95
    eps: 1.0e-8
    schedule_type: "cosine"
    min_lr_ratio: 0.1

# TPU v4-32 Configuration (from precautionfortpu.md)
tpu:
  # Mesh topology for TPU v4-32
  mesh_shape: [4, 4, 2]  # x, y, z axes
  axis_names: ["x", "y", "z"]
  
  # Sharding strategy
  sharding:
    use_2d_sharding: true
    model_parallel_axes: ["x", "y"]  # 2D tensor parallel
    data_parallel_axis: "z"
    shard_embeddings: true
    shard_s5: false  # Keep S5 replicated for stability
  
  # Mixed precision policy (critical for stability)
  mixed_precision:
    param_dtype: "float32"
    compute_dtype: "bfloat16"  # TPU-optimized
    attention_softmax_dtype: "float32"  # Critical: avoid NaNs
    s5_complex_dtype: "complex64"  # Critical: S5 stability
    layernorm_dtype: "float32"
    loss_dtype: "float32"
    grad_dtype: "float32"

# Data Pipeline Configuration
data:
  # FineWeb dataset
  dataset_name: "HuggingFaceFW/fineweb"
  dataset_config: "CC-MAIN-2024-10"  # or "sample-10BT" for testing
  split: "train"
  streaming: true
  
  # Multi-host data loading
  shard_by_host: true
  seed: 42
  
  # Processing configuration
  batch_size: 8
  num_workers: 4
  prefetch_factor: 2
  buffer_size: 1000
  
  # Tokenizer configuration
  tokenizer:
    tokenizer_name: "gpt2"
    vocab_size: 50257
    max_length: 65536
    padding_side: "right"
    truncation_side: "right"
    pad_token: "<|endoftext|>"
    eos_token: "<|endoftext|>"
    bos_token: null
    unk_token: "<|endoftext|>"

# Checkpointing Configuration
checkpointing:
  checkpoint_dir: "./checkpoints"
  save_interval_steps: 1000
  keep_checkpoints: 5
  async_save: true
  
  # Multi-level checkpointing
  fast_checkpoint_interval: 100
  full_checkpoint_interval: 1000
  
  # Validation
  validate_on_save: true
  validate_on_load: true
  use_compression: true
  compression_level: 6

# Logging Configuration
logging:
  log_level: "INFO"
  log_dir: "./logs"
  log_to_file: true
  log_to_console: true
  
  # Multi-host coordination
  aggregate_logs: true
  primary_host_only: true  # Reduce log volume
  
  # Wandb integration
  use_wandb: true
  wandb_project: "valkyrie-training"
  wandb_entity: null
  wandb_tags: ["longformer", "s5", "tpu-v4-32"]
  
  # Performance monitoring
  log_system_metrics: true
  metrics_interval: 60

# Validation and Testing
validation:
  # Unit test configuration
  run_s5_tests: true
  run_attention_tests: true
  run_gradient_tests: true
  
  # Validation intervals
  validate_every_steps: 1000
  validation_batches: 10
  
  # Numerical stability checks
  check_for_nans: true
  nan_check_interval: 100
  
  # Performance benchmarks
  benchmark_throughput: true
  benchmark_memory: true

# System Configuration
system:
  # JAX configuration
  jax_enable_x64: true   # Enable for numerical stability in S5 initialization
  jax_debug_nans: false  # Enable only for debugging
  jax_disable_jit: false  # Enable only for debugging
  
  # Memory management
  preallocate_memory: true
  memory_fraction: 0.9
  
  # Performance tuning
  enable_async_dispatch: true
  enable_fast_math: true