# TPU v4-32 Mesh Configuration
# Optimized topology and sharding for 32-chip TPU v4 pod

# Mesh Topology (from precautionfortpu.md)
mesh:
  # Device configuration
  device_count: 32
  mesh_shape: [4, 4, 2]  # 4×4×2 topology for optimal bisection bandwidth
  axis_names: ["x", "y", "z"]
  
  # Axis mapping (following TPU v4 paper recommendations)
  axis_mapping:
    x: "model_parallel_1"  # Tensor parallel width
    y: "model_parallel_2"  # Tensor parallel height  
    z: "data_parallel"     # Batch/data sharding
  
  # Topology optimization
  topology_type: "3d_torus"
  optimize_for_bisection: true
  use_twisted_torus: false  # Keep simple for initial setup

# Sharding Strategy
sharding:
  # Model parallelism
  model_parallel:
    type: "2d_tensor_parallel"
    axes: ["x", "y"]
    
    # Weight matrix sharding patterns
    dense_layers:
      attention_qkv: "2d"      # P('x', 'y') for Q/K/V projections
      attention_out: "row"     # P('x', None) for output projection
      ffn_gate_up: "2d"        # P('x', 'y') for gate/up projections
      ffn_down: "row"          # P('x', None) for down projection
    
    # Embedding sharding
    embeddings:
      vocab_embedding: "row"   # P('x') - shard vocab dimension
      position_embedding: "replicated"  # Small, keep replicated
  
  # Data parallelism
  data_parallel:
    axis: "z"
    batch_sharding: true
    gradient_reduction: "pmean"  # Average gradients across data replicas
  
  # Special handling for S5 (numerical stability)
  s5_sharding:
    parameters: "replicated"   # Keep S5 params replicated for stability
    states: "data_parallel"    # Shard S5 states along batch dimension
    complex_arithmetic: "replicated"  # Keep complex ops replicated

# Communication Patterns
communication:
  # Collective operations
  all_reduce_axis: "z"       # Data parallel axis for gradient reduction
  all_gather_axes: ["x", "y"]  # Model parallel axes for parameter gathering
  
  # Optimization
  fuse_collectives: true
  overlap_compute_comm: true
  
  # Bandwidth optimization (3D torus specific)
  prefer_ring_reduce: false
  use_hierarchical_reduce: true

# Memory Management
memory:
  # Per-chip memory (TPU v4 has ~32 GiB HBM per chip)
  hbm_per_chip_gb: 32
  
  # Memory allocation strategy
  preallocate_memory: true
  memory_fraction: 0.9
  
  # Activation checkpointing
  gradient_checkpointing: true
  checkpoint_policy: "minimal"  # Checkpoint minimal set for memory
  
  # Parameter sharding for memory efficiency
  shard_large_params: true
  param_size_threshold_mb: 100

# Performance Tuning
performance:
  # Compilation
  enable_jit: true
  jit_cache_size: 1000
  
  # XLA optimizations
  enable_xla_optimizations: true
  xla_dump_to: null  # Set path for XLA debugging
  
  # TPU-specific optimizations
  enable_async_dispatch: true
  enable_fast_math: true
  use_bfloat16_matmuls: true
  
  # Profiling
  enable_profiling: false
  profile_steps: [100, 200, 300]  # Steps to profile

# Fault Tolerance
fault_tolerance:
  # Checkpoint frequency for fault recovery
  checkpoint_every_minutes: 30
  
  # Host failure handling
  enable_host_failure_recovery: true
  max_host_failures: 2
  
  # Preemption handling
  handle_preemption: true
  preemption_grace_period: 300  # seconds

# Monitoring and Debugging
monitoring:
  # System monitoring
  monitor_memory_usage: true
  monitor_communication: true
  monitor_throughput: true
  
  # Alerts
  memory_usage_threshold: 0.95
  throughput_drop_threshold: 0.8
  
  # Debugging
  debug_mode: false
  debug_print_shapes: false
  debug_nan_checks: false  # Enable only when debugging NaNs

# Environment Variables (for TPU setup)
environment:
  # JAX configuration
  JAX_PLATFORMS: "tpu"
  JAX_ENABLE_XLA_PYTHON_CLIENT_ALLOCATOR: "false"
  
  # TPU-specific
  TPU_CHIPS_PER_HOST_BOUNDS: "2,2,1"  # Adjust based on actual topology
  TPU_HOST_BOUNDS: "2,2,2"            # Adjust based on actual topology
  
  # Performance
  XLA_FLAGS: "--xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true"